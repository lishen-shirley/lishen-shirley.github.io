<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html><head><meta http-equiv="Content-Type" content="text/html; charset=windows-1252">
  <script type="text/javascript" async="" src="./files/analytics.js"></script><script async="" src="./files/js"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }
    gtag('js', new Date());
    gtag('config', 'UA-7580334-1');
  </script>
  <meta name="viewport" content="width=500">
  <link href="./files/stylesheet.css" rel="stylesheet" type="text/css">
  <title>Li Shen</title>
  
  <link href="./files/css" rel="stylesheet" type="text/css">
</head>

<body>
  <table width="950" border="0" align="center" cellspacing="0" cellpadding="0">
    <tbody><tr>
      <td>
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tbody><tr>
            <td width="5%">
            </td> 
            <td width="30%">
              <img src="./files/lishen_photo.jpg" height="256">
            </td>
            <td width="65%" valign="middle">
              <p align="center">
                <name>Li Shen (申丽)</name>
              </p>
              <p align="center">
                <img src="files/email_icon.png" width="30" align="absbottom">
<a href="mailto:li.shen.shirley@gmail.com">li.shen.shirley@gmail.com</a>
              <p>
                I'm a senior researcher at <a href="https://ai.tencent.com/ailab/en/index">Tencent AI Lab</a>. Formerly I was a researcher in the <a href="http://www.robots.ox.ac.uk/~vgg/">Visual Geometry Group</a> at the University of Oxford, working with <a href="https://scholar.google.com/citations?user=UZ5wscMAAAAJ&amp;hl=en">Professor Andrew Zisserman</a>. </p>

              <p>
                I received my Ph.D in Computer Science from the University of Chinese Academy of Sciences, under the supervision of <a href="https://scholar.google.com/citations?user=J1vMnRgAAAAJ&hl=en">Prof. Qingming Huang</a>. I was also supervised by <a href="http://www.cis.pku.edu.cn/faculty/vision/zlin/zlin.htm">Prof. Zhouchen Lin</a> at the Peking University.
<br> My research interests are in computer vison and deep learning.
              <p align="left">
                <a href="https://scholar.google.com/citations?user=ABbCaxsAAAAJ&hl=en">Google Scholar</a> &nbsp;/&nbsp;
                <a href="https://www.linkedin.com/in/li-shen-14407949/"> LinkedIn </a>
              </p>
            </td>
          </tr>
        </tbody>
        </table>

        <img src="files/news_icon.png" width="35px" align="absbottom"> 
        <heading>News</heading><p>
        <ul style="list-style-type:circle;">
          <li style="margin: 8px 0;"><font color="red">Recruiting researchers/engineers/research interns!</font> Feel free to <a href="mailto:li.shen.shirley@gmail.com">contact me</a>.</li>
          <li style="margin: 8px 0;"> I'm the organiser of the workshop <a href="https://neuralarchitects.org/">Neural Architects</a> at <a href="http://iccv2019.thecvf.com/program/workshops">ICCV 2019</a>.
          <p><strong>Neural Architects: What have we learned and where are we going?</strong> - for all things related to Deep Neural Network design. <br>
Call for papers and additional info <a href="https://neuralarchitects.org/">here</a>.</p> </li>
        </ul>
        </p>
        <br>
        <img src="files/publication_icon.png" width="35px" align="absbottom"> 
        <heading>Research</heading><p>
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="12">
        <tbody><tr> 
        <td width="25%"><img src="./files/se_journal_icon.png" width="200" height="100">
          </td>
         <td width="75%" valign="middle"> 
        <a href="https://ieeexplore.ieee.org/document/8701503">
                <papertitle>Squeeze-and-Excitation Networks</papertitle>
              </a><br>Jie Hu, <strong>Li Shen</strong>, Samuel Albanie, Gang Sun and Enhua Wu<br>
              <em>IEEE Transactions on Pattern Analysis and Machine Intelligence</em>, 2019.<br>
           Journal extension of the work 
           <a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Hu_Squeeze-and-Excitation_Networks_CVPR_2018_paper.pdf">SENets</a>. <br>
           <a href="https://ieeexplore.ieee.org/document/8701503">Paper</a> / <a href="https://arxiv.org/pdf/1709.01507.pdf">ArXiv</a> /
              <a href="https://github.com/hujie-frank/SENet">Code &amp; Model</a> /
              <a href="bibtex/hu_pami19.bib" download="hu_pami19.bib">Bibtex</a>
          </td>
          </tr>
          <tr> 
        <td width="25%"><img src="./files/ge_icon.png" width="210">
          </td>
         <td width="75%" valign="middle"> 
        <a href="https://ieeexplore.ieee.org/document/8701503">
                <papertitle>Gather-Excite: Exploiting Feature Context in Convolutional Neural Networks</papertitle>
              </a><br />Jie Hu*, <strong>Li Shen</strong>*, Samuel Albanie*, Gang Sun and Andrea Vedaldi<br />
              <em>Advances in Neural Information Processing Systems</em> (NeurIPS), 2018.<br />
           <a href="https://papers.nips.cc/paper/8151-gather-excite-exploiting-feature-context-in-convolutional-neural-networks.pdf">Paper</a> / <a href="https://arxiv.org/pdf/1810.12348.pdf">ArXiv</a> /
              <a href="https://github.com/hujie-frank/GENet">Code &amp; Model</a> / <a href="bibtex/hu_nips18.bib" download="hu_nips18.bib">Bibtex</a>
          </td>
          </tr>
        <tr> 
        <td width="25%"><img src="./files/comparator_icon.png" width="200">
          </td>
         <td width="75%" valign="middle"> 
          <a href="http://openaccess.thecvf.com/content_ECCV_2018/papers/Weidi_Xie_Comparator_Networks_ECCV_2018_paper.pdf">
                <papertitle>Comparator Networks</papertitle>
          </a><br>Weidi Xie, <strong>Li Shen</strong>, Andrew Zisserman
              <br>
              <em>European Conference on Computer Vision (ECCV)</em>, 2018.
              <br>
              <a href="http://openaccess.thecvf.com/content_ECCV_2018/papers/Weidi_Xie_Comparator_Networks_ECCV_2018_paper.pdf">Paper</a> / <a href="https://arxiv.org/abs/1807.11440">ArXiv</a> / <a href="bibtex/xie_eccv18.bib" download="xie_eccv18.bib">Bibtex</a>
          </tr>
          <tr> 
        <td width="25%"><img src="./files/se_icon.png" width="220">
          </td>
         <td width="75%" valign="middle"> 
        <a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Hu_Squeeze-and-Excitation_Networks_CVPR_2018_paper.pdf">
          <papertitle>Squeeze-and-Excitation Networks</papertitle> </a>
              <br>Jie Hu*, <strong>Li Shen</strong>* and Gang Sun<br>
              <em>Computer Vision and Pattern Recognition (CVPR)</em>, 2018 <strong>(Oral)</strong>.<br>
           <strong>Winner at the <a href="http://image-net.org/challenges/LSVRC/2017/results">ImageNet (ILSVRC) 2017</a> Image Classification</strong> Track.<br>
           <a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Hu_Squeeze-and-Excitation_Networks_CVPR_2018_paper.pdf">Paper</a> /
              <a href="https://github.com/hujie-frank/SENet">Code &amp; Model</a> /
              <a href="bibtex/hu_cvpr18.bib" download="hu_cvpr18.bib">Bibtex</a>
          </td>
          </tr>
        <tr> 
        <td width="25%"><img src="./files/vggface2_icon.png" width="200">
          </td>
         <td width="75%" valign="middle"> 
        <a href="https://arxiv.org/pdf/1710.08092">
          <papertitle>Vggface2: A dataset for recognising faces across pose and age</papertitle> </a>
              <br>Qiong Cao, <strong>Li Shen</strong>, Weidi Xie, Omkar M Parkhi and Andrew Zisserman<br>
              <em>Conference on Automatic Face & Gesture Recognition (FG)</em>, 2018 <strong>(Oral)</strong>.<br>
           <a href="https://arxiv.org/pdf/1710.08092">ArXiv</a> /
              <a href="https://github.com/ox-vgg/vgg_face2">Code &amp; Model</a> / 
           <a href="http://www.robots.ox.ac.uk/~vgg/data/vgg_face2/">Project</a> / 
              <a href="bibtex/cao_fg18.bib" download="cao_fg18.bib">Bibtex</a>
          </td>
          </tr>
        <tr> 
        <td width="25%"><img src="./files/relaybp_icon.png" width="200">
          </td>
         <td width="75%" valign="middle"> 
        <a href="https://rd.springer.com/chapter/10.1007/978-3-319-46478-7_29">
          <papertitle>Relay Backpropagation for Effective Learning of Deep Convolutional Neural Networks</papertitle> </a>
              <br> <strong>Li Shen</strong>, Zhouchen Lin and Qingming Huang<br>
              <em>European conference on computer vision (ECCV)</em>, 2016.<br>
           <strong>Winner at the <a href="http://image-net.org/challenges/LSVRC/2015/results">ImageNet (ILSVRC) 2015</a> Scene Classification</strong> Track.<br>
           <a href="https://rd.springer.com/chapter/10.1007/978-3-319-46478-7_29">Paper / 
           <a href="https://arxiv.org/pdf/1512.05830.pdf">ArXiv</a> /
              <a href="https://github.com/lishen-shirley/Places2-CNNs">Model</a> / 
              <a href="bibtex/shen_eccv16.bib" download="shen_eccv16.bib">Bibtex</a>
          </td>
          </tr>
        <tr> 
        <td width="25%"><img src="./files/skeleton_lstm_icon.png" width="200">
          </td>
         <td width="75%" valign="middle"> 
        <a href="https://arxiv.org/pdf/1603.07772.pdf">
          <papertitle>Co-occurrence Feature Learning for Skeleton Based Action Recognition Using Regularized Deep LSTM Networks</papertitle> </a>
              <br>Wentao Zhu, Cuiling Lan, Junliang Xing, Wenjun Zeng, Yanghao Li, <strong>Li Shen</strong> and Xiaohui Xie<br>
              <em>Thirtieth AAAI Conference on Artificial Intelligence</em>, 2016.<br>
           <a href="https://dl.acm.org/citation.cfm?id=3016423">Paper / 
           <a href="https://arxiv.org/pdf/1603.07772.pdf">ArXiv</a> / 
              <a href="bibtex/zhu_aaai16.bib" download="zhu_aaai16.bib">Bibtex</a>
          </td>
          </tr>
        <tr> 
        <td width="25%"><img src="./files/mlddl_journal_icon.png" width="200">
          </td>
         <td width="75%" valign="middle"> 
        <a href="https://ieeexplore.ieee.org/abstract/document/7113864">
          <papertitle>Multi-Level Discriminative Dictionary Learning with Spplication to Large Scale Image Classification</papertitle> </a>
           <br><strong>Li Shen</strong>, Gang Sun, Qingming Huang, Shuhui Wang, Zhouchen Lin and Enhua Wu<br>
              <em>IEEE Transactions on Image Processing</em>, 2016.<br>
           <a href="http://www.jdl.link/doc/2011/20161191011898744_tip15-shen.pdf">Paper / 
              <a href="bibtex/shen_tip15.bib" download="shen_tip15.bib">Bibtex</a>
          </td>
          </tr>
        <tr> 
        <td width="25%"><img src="./files/share_icon.png" height="120">
          </td>
         <td width="75%" valign="middle"> 
        <a href="http://www.cis.pku.edu.cn/faculty/vision/zlin/Publications/2015-IJCAI-Adaptive.pdf">
          <papertitle>Adaptive Sharing for Image Classification</papertitle> </a>
           <br><strong>Li Shen</strong>, Gang Sun, Zhouchen Lin, Qingming Huang and Enhua Wu<br>
              <em>International Joint Conference on Artificial Intelligence (IJCAI)</em>, 2015.<br>
           <a href="https://www.ijcai.org/Proceedings/15/Papers/309.pdf">Paper / 
              <a href="bibtex/shen_ijcai15.bib" download="shen_ijcai15.bib">Bibtex</a>
          </td>
          </tr>
        <tr> 
        <td width="25%"><img src="./files/mlddl_icon.png" width="200">
          </td>
         <td width="75%" valign="middle"> 
        <a href="http://openaccess.thecvf.com/content_cvpr_2013/papers/Shen_Multi-level_Discriminative_Dictionary_2013_CVPR_paper.pdf">
          <papertitle>Multi-Level Discriminative Dictionary Learning towards Hierarchical Visual Categorization</papertitle> </a>
           <br><strong>Li Shen</strong>, Gang Sun, Zhouchen Lin, Qingming Huang and Enhua Wu<br>
              <em>Computer Vision and Pattern Recognition (CVPR)</em>, 2013.<br>
           <a href="http://openaccess.thecvf.com/content_cvpr_2013/papers/Shen_Multi-level_Discriminative_Dictionary_2013_CVPR_paper.pdf">Paper / 
              <a href="bibtex/shen_cvpr13.bib" download="shen_cvpr13.bib">Bibtex</a>
          </td>
          </tr>
  </tbody></table>
  <br>
  <br>
  <img src="files/competition_icon.png" width="40" align="absbottom">
  <heading> Academic Competitions </heading>
          <ul style="list-style-type:circle;">
          <li style="margin: 8px 0;">
          Winner at the <a href="http://image-net.org/challenges/LSVRC/2017/results">ImageNet (ILSVRC) 2017</a> Image Classification Track (<a href="http://image-net.org/challenges/beyond_ilsvrc">CVPR17 Workshop</a>). </li>
          <li style="margin: 8px 0;">
          Winner at the <a href="http://image-net.org/challenges/LSVRC/2015/results">ImageNet (ILSVRC) 2015</a> Scene Classification Track (<a href="http://image-net.org/challenges/ilsvrc+mscoco2015">ICCV15 Workshop</a>). </li>
          </ul>
          <br>
          <br>
          <br>
         <p align="right">Last Update: July 1, 2019</p>
         <p align="right">Published with <a href="https://pages.github.com">GitHub Pages</a></p>
    </tbody>
    </table>
          <br>
          <br>
</body></html>
